<!DOCTYPE html>
<html lang="en">

<head>
    <title>ProScale</title>

    <!-- seo -->
    <meta name="author" content="Michael Crum, Zehui Lin (zl883)">
    <meta name="description" content="ProScale - Smart, Contact Free Kitchen Scale">
    <meta name="keywords" content="robotics">

    <!-- display -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">

    <!-- icon -->
    <link rel="icon" type="image/png" sizes="32x32" href="../global_assets/icons/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="../global_assets/icons/favicon-16x16.png" />

    <!-- stylesheets -->
    <link rel="stylesheet" href="./styles/index.css">
    <link rel="stylesheet" href="./styles/highlight/styles/base16/bright.min.css">
    <link rel="stylesheet" href="./styles/katex/katex.min.css">

    <!-- syntax highlighting-->
    <script src="../styles/highlight/highlight.min.js"></script>
    <script src="../styles/highlight/languages/verilog.min.js"></script>
    <script>
        hljs.highlightAll();
    </script>

    <!-- latex support-->
    <script defer src="../styles/katex/katex.min.js"></script>

    <!-- font -->
    <link href="https://fonts.googleapis.com/css2?family=Pacifico&display=swap" rel="stylesheet" />

    <!-- iFrame optimization -->
    <script>
        checkVisibility();
        document.addEventListener("scroll", (event) => {
            checkVisibility();
        });

        function checkVisibility() {
            const frames = document.getElementsByTagName("iframe");
            for (var i = 0; i < frames.length; i++) {
                frame = frames[i];
                if (isInViewport(frame)) {
                    frame.style.visibility = "visible";
                } else {
                    frame.style.visibility = "hidden";
                }
            }
        }

        function isInViewport(elm) {
            var rect = elm.getBoundingClientRect();
            var viewHeight = Math.max(document.documentElement.clientHeight, window.innerHeight);
            return !(rect.bottom < 0 || rect.top - viewHeight >= 0);
        }
    </script>
</head>

<body>
    <article id="article">
        <h1>ProScale - A Smart, Gesture Controlled, Contact Free Kitchen Scale</h1>
<p><em>By Michael Crum (mmc323@cornell.edu) and Zehui Lin (zl883)</em></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/et91Gea6CPk?si=lng-3xK4TcgsFHiM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
<h2>Objective</h2>
<p>For our final project we developed a kitchen scale with gesture-based controls using a Raspberry Pi, a load cell with an HX711 sensor, and a Pi TFT display.
Motivated by the challenges of maintaining hygiene in the kitchen, we set out to create a hands-free solution that also enhances accessibility for individuals with limited mobility.
Users can easily weigh ingredients, switch units, and navigate recipes through a gesture-based interface on the Pi TFT screen, enhancing the kitchen prep experience.</p>
<h2>Introduction</h2>
<p>This project integrates hardware and software to build a gesture-controlled kitchen scale.
At its core, the system utilizes a Raspberry Pi for processing, with a Pi Camera for real-time video capture, a load cell with HX711 sensor for weight measurements, and a Pi TFT display serving as the user interface, all housed in a custom laser-cut case.
The captured video is pre-processed using OpenCV for frame flipping, color conversion, and scaling, followed by MediaPipe for detecting 21 hand landmarks.
By analyzing the positions and distances of these landmarks, gestures like pinching and pointing are identified and mapped to actions like navigating recipes, switching units, and the tare function.
The interface, developed using PyGame, dynamically updates in response to cursor movement and click events, providing a touch-free, hygienic, and accessible user experience.</p>
<h2>High-Level System Design</h2>
<p><img src="./assets/high_level_design.webp" alt=""></p>
<blockquote>
<p>Figure 1: High-Level System Design</p>
</blockquote>
<h2>Hardware Setup</h2>
<img src="./assets/full_system.webp" style="width: 40%; display: inline-block"/>
<img src="./assets/cad.webp" style="width: 58%; display: inline-block"/>
<blockquote>
<p>Figure 2: Final Hardware vs CAD</p>
</blockquote>
<p>The functional components of our system are the Raspberry Pi, the PiTFT, the Pi Camera, and the load cell.
To connect these elements into a convenient package, we designed and laser cut an enclosure for the components.</p>
<h2>OpenCV and MediaPipe Implementation</h2>
<p><img src="./assets/hand_landmarks.webp" alt=""></p>
<blockquote>
<p>Figure 3: Hand Landmarks</p>
</blockquote>
<p>MediaPipe is a collection of tools for applying machine learning models to various computer vision tasks.
For ProScale we used MediaPipe's hand landmarks model.
This model identifies hands within an input image and extracts the location of each joint.
See Figure 3 for a diagram of the information generated by MediaPipe.
Each landmark has a corresponding x and y location in screen space, as well as an estimate of z - the depth of the landmark.</p>
<p>We used a Pi Camera module to capture images for MediaPipe.
This was an easy choice because Pi Camera is widely used and very well documented.
We installed the picamera python driver using <code>sudo apt install -y python3-picamera2</code>.
We then ran a simple test program to open a preview window:</p>
<pre><code class="language-python">from picamera2 import Picamera2, Preview
import time
picam2 = Picamera2()
camera_config = picam2.create_still_configuration(main={&quot;size&quot;: (1920, 1080)}, lores={&quot;size&quot;: (640, 480)}, display=&quot;lores&quot;)
picam2.configure(camera_config)
picam2.start_preview(Preview.QTGL)
picam2.start()
time.sleep(60)
</code></pre>
<p>This example requires an X server connection and the <code>QTGL</code> driver to be available.
This is only the case when running the graphical desktop through the RPi’s HDMI port, and doesn’t work over an SSH connection.
To run the example over SSH, it is possible to forward the local X server using <code>ssh -x</code>, although the QTGL driver won’t be available. Instead, you’ll need to switch <code>Preview.QTGL</code> to <code>Preview.QT</code>.</p>
<p>Once the camera's functionality was confirmed, we looked for a way to display the camera’s output on the TFT’s screen. QT provides a frame buffer driver, but we were unable to make it function in our testing. Instead, we chose to write directly to the TFT’s frame buffer. This can be achieved as shown in the following snippet:</p>
<pre><code class="language-python">frame = picam2.capture_array(&quot;lores&quot;)
image = cv2.cvtColor(frame, cv2.COLOR_YUV420p2BGR)
image = cv2.resize(image, (320, 240))

image = cv2.cvtColor(image, cv2.COLOR_BGR2BGR565)
with open(&quot;/dev/fb0&quot;, &quot;rb+&quot;) as buf:
    buf.write(image)

</code></pre>
<p>With the picam up and running, we installed MediaPipe using <code>sudo pip install mediapipe</code> (<code>sudo</code> for compatibility with PyGame, which requires root to display on the TFT).
We were able to display an annotated diagram from the hand recognition model using the following snippet:</p>
<pre><code class="language-python">with mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.8, min_tracking_confidence=0.8, max_num_hands=1) as hands:
    while True:
        frame = picam2.capture_array(&quot;lores&quot;)
        image = cv2.cvtColor(frame, cv2.COLOR_YUV420p2BGR)
        detected_image = hands.process(image)
        image = cv2.resize(image, (320, 240))

        if detected_image.multi_hand_landmarks:
            for hand_lms in detected_image.multi_hand_landmarks:
                mp_drawing.draw_landmarks(
                    image,
                    hand_lms,
                    mp_hands.HAND_CONNECTIONS,
                    landmark_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(
                        color=(255, 0, 255), thickness=4, circle_radius=2
                    ),
                    connection_drawing_spec=mp.solutions.drawing_utils.DrawingSpec(
                        color=(20, 180, 90), thickness=2, circle_radius=2
                    ),
                )

        image = cv2.cvtColor(image, cv2.COLOR_BGR2BGR565)
        with open(&quot;/dev/fb0&quot;, &quot;rb+&quot;) as buf:
            buf.write(image)

        if cv2.waitKey(1) &amp; 0xFF == ord(&quot;q&quot;):
            break
</code></pre>
<p>The hand landmarks include information about the location of each joint in the detected hand.
We extract the tip of the thumb <code>hand_lms[4]</code> and used that as the cursor position for interaction.
We then extract the tip of the pointer finger <code>hand_lms[8]</code> and compare the location of the thumb and pointer finger the determine when the user is clicking. If the distance between the thumb and pointer finger is &lt; 0.1, a click is registered and the relevant callback is executed. In code, this looks like:</p>
<pre><code class="language-python">thumb_landmark = hand_lms.landmark[4]
pointer_landmark = hand_lms.landmark[8]

self.cursor_render_callback(1-thumb_landmark.x, 1-thumb_landmark.y)

if (thumb_landmark.x - pointer_landmark.x) ** 2 + (
    thumb_landmark.y - pointer_landmark.y
) ** 2 &lt; 0.01 and self.click_callback:
    if self.clicking == False:
        self.click_callback(1-thumb_landmark.x, 1-thumb_landmark.y)
    self.clicking = True
else:
    self.clicking = False
</code></pre>
<h2>Weight Measurement</h2>
<p>The weight measurement module combines the HX711 sensor, the 5kg load cell, and the Raspberry Pi to obtain weight readings.
The hardware configuration began with connecting the HX711's data (DT) and clock (SCK) pins to GPIO 5 and 6, respectively, and connecting three physical buttons on piTFT to GPIO pins 17, 22, and 27 for critical functions such as tare, unit switching, and quit.
These buttons were added as a convenient and straightforward interface to streamline the testing process.</p>
<p>With the setup complete, we began testing the weight measurement module with the library <code>clone git@github.com:tatobari/hx711py.git</code>.
However, when running the example code, we encountered challenges with obtaining valid weight readings. Despite verifying that the power supply, DT, and SCK pins were functioning correctly using a multimeter, we observed no small voltage changes across the signal wires (A+ and A-) when weight was applied.
Further multimeter diagnostics indicated abnormally low resistance between A+ and A-, indicating a potential fault in the load cell itself.
To address this, we replaced the load cell with a new component.
We then tested the system again, confirmed stable voltage across the signal and excitation wires, and successfully obtained weight measurements.
Then we calibrated the system using a 100g reference weight to further refine the weight measurement process.</p>
<p>Another issue arose due to the time-sensitive nature of polling bits from the HX711 on the Raspberry Pi.
The Raspberry Pi running on the Linux-based operating system may prioritize other tasks over GPIO operations, leading to occasional random or noisy values in the weight readings.
To mitigate this issue, we initially implemented an exponential smoothing method to stabilize the output, which calculates a weighted average of the current reading and the previous smoothed value using the following formula:</p>
<p>Smoothed Value=<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>×</mo></mrow><annotation encoding="application/x-tex">\alpha \times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mord">×</span></span></span></span></eq>Current Value<eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>α</mi><mo stretchy="false">)</mo><mo>×</mo></mrow><annotation encoding="application/x-tex">+(1−\alpha)\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">+</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mclose">)</span><span class="mord">×</span></span></span></span></eq>Previous Smoothed Value</p>
<p>where <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></eq> is the smoothing factor.
However, this approach did not perform as expected, possibly due to the frequent large noise fluctuations, regardless of the choice of <eq><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span></span></span></span></eq>.</p>
<p>We then used a median filter to reduce the effect.
A buffer was used to store the ten most recent readings, and the median of these values was returned as the current weight.
This sliding window approach effectively removed large outliers and provided a relatively consistent measurement.
It also ensured that random fluctuations had less impact while still allowing the system to adapt to real weight changes.</p>
<p>Functional testing verified the proper operation of all core features.
The tare function can reset the weight to zero by taking the current raw reading from the sensor and setting it as the offset.
And the unit toggle enables switching between grams and ounces using the setUnit() function, with the conversion handled in the rawBytesToWeight() function by multiplying the weight in grams by the conversion factor 0.035274 when switched to ounces.
And the quit button enabled a smooth shutdown with GPIO cleanup.</p>
<h2>Menu Design</h2>
<h3>Initial Design</h3>
<p>The initial design of the menu was structured around simplicity and intuitive navigation.
It included a Main Menu with two primary options: Scale Mode and Recipe Mode.
Scale Mode provided real-time weight measurements, as well as unit conversion and tare capability.
Recipe Mode allowed users to browse recipes and follow step-by-step instructions, with navigation buttons for switching between recipes and steps.
Upon completion of a recipe, users were prompted with a feedback screen to like or dislike the recipe.</p>
<p><img src="./assets/initial_state_machine.webp" alt=""></p>
<blockquote>
<p>Figure 4: Initial Menu Design</p>
</blockquote>
<h3>Final Design</h3>
<p>In our final design, we chose to incorporate the scale mode directly into the home page and orient the piTFT screen vertically.
By combining the scale mode with the home screen, users can perform weight measurements, tare, and unit switching without navigating away, streamlining functionality and reducing complexity.
The vertical orientation further improves usability by optimizing the display for button and reading flow, as well as allowing for larger text and images, making it easier to follow instructions and see real-time measurements at a glance.</p>
<p>The Home page is the initial interface displayed upon starting the system, providing access to core scale functions and navigation options.
It prominently shows real-time weight readings from the HX711 sensor at the bottom, which is dynamically updated by the draw() function The top section features buttons for tare and unit switching, which can be activated through gestures detected by the check_click(), triggering respective functions such as self.hx.tare() to reset the scale and self.hx.setUnit() to toggle between grams and ounces. And the Recipes button directs users to the recipe page.</p>
<p>On the Recipe Home page, the bottom area displays an image of the stored recipe, loaded dynamically from json files using pygame.image.load and scaled with pygame.transform.rotozoom.
The top section includes a Home button, allowing users to return to the home page. And users can cycle through the recipes by “clicking” the “&lt;&lt;&lt;” and ”&gt;&gt;&gt;” buttons. The check_click() function handles navigation by updating the self.recipe_index variable, while the draw function ensures that the image is rendered in real-time.</p>
<p>The interface on the Recipe Details page combines step-by-step recipe instructions with real-time weight measurements to streamline the cooking process. The topmost part contains the Home and Return buttons that allow users to return to the home page or recipe home page, respectively. Weight measurements with the unit are shown below, along with Tare and Unit buttons, like on the Home page. And the recipe instructions are loaded from json files at the bottom. The instructions are dynamically rendered and are updated when a pinching gesture is detected, which helps users easily progress to the next step without touching the device. Specifically, pinching left moves to the previous step, while pinching right progresses to the next step. By integrating real-time weight measurements with dynamically updated instructions, the Recipe Details page provides an intuitive and efficient user experience.</p>
<img src="./assets/scale_home.webp" style="width: 30%; display: inline-block"/>
<img src="./assets/recipe_home.webp" style="width: 30%; display: inline-block"/>
<img src="./assets/recipe_details.webp" style="width: 30%; display: inline-block"/>
<blockquote>
<p>Figure 5: Home Page, Recipe Home Page, and Recipe Details Page (from left to right). The blue point indicates the gesture-detected cursor.</p>
</blockquote>
<h3>Integration</h3>
<p>We wrote a modular GUI system to manage the state and transitions of our application.
The system is controlled by the GUI class, which provides the <code>add_gui_element</code> method.
This method accepts a class instance as an argument, with the expectation that this class implements the following functions:</p>
<ul>
<li><code>draw(self, surface, state: GUIState, state_info) -&gt; None</code></li>
<li><code>check_click(self, position: Tuple(x, y), state: GUIState, state_info) -&gt; (State, state_info)</code></li>
</ul>
<p>Each frame the GUI class loops over all registered GUI elements and calls their draw method, which conditionally renders elements depending on the current state of the application.
When a click input is received from the MediaPipe hand detection driver, check_click is called on all GUI elements. Each element handles the click and has the option to return a new <code>GUIState</code>, representing a state transition within the GUI state machine. An example would be clicking the &quot;Recipes&quot; button on the home screen - in the <code>SCALE_HOME</code> state the HomeScreen GUI element would recognize that the button was pressed and return <code>RECIPE_HOME</code> to represent a transition to a new page.</p>
<p>Using this system we are able to write reusable GUI elements without brittle and hand coded interaction logic.</p>
<p>Finally, app.py was implemented to integrate all components, connecting the GUI, gesture recognition, and hardware functionalities.
The script first initializes the piTFT display, the Interaction module for gesture processing, and the GUI object.
The GUI classes' callbacks for cursor motion and click handling are registered with the interaction class using <code>interface.register_cursor_render_callback</code> and <code>interface.register_click_callback</code>.</p>
<p>To ensure smooth operation, app.py makes use of multithreading, with one thread processing gesture inputs in real-time and another updating the piTFT display at 30 frames per second using the tick() function.
By combining these aspects, app.py provides rapid screen transitions and cohesive interaction across all components, giving a scalable, efficient, and intuitive user experience.</p>
<h2>Results</h2>
<p>Overall, we’ve achieved the goals outlined in the description and delivered a functional prototype that integrated gesture-based controls, weight measurement, and recipe navigation.
Core functionalities were all successfully implemented and tested. Gesture detection powered by Picamera and MediaPipe successfully identified hand gestures such as pinching and pointing, enabling precise and intuitive interactions.
The weighting function, driven by the HX711 sensor and a 5kg load cell, provided accurate and stable weight readings after resolving initial hardware issues and implementing a robust median filter.
The GUI design, which includes a home page, a recipe home page, and a recipe details page, simplifies users' prep experience while ensuring hygiene and accessibility, with the piTFT seamlessly updating the interface in real time.</p>
<h2>Future Work</h2>
<p>We would also integrate a microcontroller to handle the precise bit-polling required by the HX711 sensor, reducing the impact of the Raspberry Pi's multitasking on measurement reliability. We would also have provided a feedback page at the end of each recipe, allowing users to like or dislike recipes, as well as the ability to customize ingredient quantities to suit their preferences or serving sizes.</p>
<h2>Budget</h2>
<ul>
<li>Raspberry Pi 4 &amp; piTFT - Provided in the lab</li>
<li>Raspberry Pi Camera - Provided in the lab</li>
<li>5kg Load Cell &amp; HX711 Weight Sensor - $6</li>
</ul>
<h2>Contribution</h2>
<p><img src="./assets/team_photo.webp" alt=""></p>
<p>Michael implemented the camera interaction / gesture recognition features.
He also designed and laser cut the enclosure.</p>
<p>Zehui implemented the load cell driver and filtering algorithm.</p>
<p>Both worked on the GUI and UI state machine.</p>
<h2>Code Appendix</h2>
<p>All code can be found in the <a href="https://github.com/usedhondacivic/proscale">GitHub repo</a>.</p>
<h2>Reference</h2>
<h3>Software</h3>
<p>MediaPipe:</p>
<ul>
<li><a href="https://pypi.org/project/mediapipe-rpi4/">https://pypi.org/project/mediapipe-rpi4/</a></li>
<li><a href="https://mediapipe.readthedocs.io/en/latest/solutions/hands.html">https://mediapipe.readthedocs.io/en/latest/solutions/hands.html</a></li>
</ul>
<p>OpenCV</p>
<ul>
<li><a href="https://raspberrypi-guide.github.io/programming/install-opencv">https://raspberrypi-guide.github.io/programming/install-opencv</a></li>
<li><a href="https://opencv.org/">https://opencv.org/</a></li>
</ul>
<p>HX711</p>
<ul>
<li><a href="https://github.com/tatobari/hx711py/tree/master">https://github.com/tatobari/hx711py/tree/master</a></li>
</ul>
<h3>Hardware</h3>
<p>PiCamera</p>
<ul>
<li><a href="https://picamera.readthedocs.io/en/release-1.13/index.html">https://picamera.readthedocs.io/en/release-1.13/index.html</a></li>
<li><a href="https://projects.raspberrypi.org/en/projects/getting-started-with-picamera">https://projects.raspberrypi.org/en/projects/getting-started-with-picamera</a></li>
</ul>
<p>HX711 sensor &amp; Load Cell</p>
<ul>
<li><a href="https://tutorials-raspberrypi.com/digital-raspberry-pi-scale-weight-sensor-hx711/#google_vignette">https://tutorials-raspberrypi.com/digital-raspberry-pi-scale-weight-sensor-hx711/#google_vignette</a></li>
</ul>
<h3>Past Student Projects</h3>
<p>Gesture Home</p>
<ul>
<li><a href="https://courses.ece.cornell.edu/ece5990/ECE5725_Fall2023_Projects/4%20Friday%20December%208/5%20Gesture%20Home%20/GestureHome_web/index.html">https://courses.ece.cornell.edu/ece5990/ECE5725_Fall2023_Projects/4%20Friday%20December%208/5%20Gesture%20Home%20/GestureHome_web/index.html</a></li>
</ul>

    </article>
    <div id="right_pad"></div>
</body>

</html>
